{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cài đặt thư viện \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import mean_absolute_percentage_error,mean_squared_error,mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data_src):\n",
    "  df = pd.read_csv(data_src, parse_dates=True,\n",
    "                       index_col=0).sort_values(by='Date', ascending=True)\n",
    "  #Xóa dấu , và chuyển về float\n",
    "  df['Price']=df['Price'].replace(',','',regex=True).astype(float)\n",
    "  \n",
    "  df = df[['Price']]\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df,scaler):\n",
    "  # 3. Scaler data\n",
    "  df1=df.reset_index()['Price']\n",
    "  df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
    "  return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lấy số lượng dữ liệu của tập train, test, validation\n",
    "def get_split_size(df1, train_ratio, test_ratio):\n",
    "    train_size = int(train_ratio * len(df1))\n",
    "    test_size = int(test_ratio * len(df1))\n",
    "    val_size = len(df1) - train_size - test_size\n",
    "    return train_size, test_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia dữ liệu\n",
    "def split_data(df1,train_size, test_size):\n",
    "  train_data = df1[:train_size]\n",
    "  test_data = df1[train_size:train_size+test_size]\n",
    "  val_data = df1[train_size+test_size:]\n",
    "  return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hàm Create Dataset\n",
    "import numpy\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step-1):\n",
    "\t\ta = dataset[i:(i+time_step), 0]   ###i=0, X=0,1,2,3-----99   Y=100 \n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(train_data,test_data,val_data,time_step):\n",
    "    \n",
    "    #6. Reshape into X=t,t+1,t+2..t+99 and Y=t+100\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_val, yval = create_dataset(val_data, time_step)\n",
    "    X_test, ytest = create_dataset(test_data, time_step)\n",
    "\n",
    "    # 7. Reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],X_val.shape[1] , 1)\n",
    "    return X_train, y_train, X_test, ytest, X_val, yval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define LSTM Model\n",
    "def build_model(time_step,unit_model):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(unit_model,activation='relu',return_sequences=True,input_shape=(time_step, 1)))\n",
    "    model.add(SimpleRNN(unit_model,activation='relu',return_sequences=True))\n",
    "    model.add(SimpleRNN(unit_model,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,X_train,y_train,X_test,ytest):\n",
    "    # 9. Fit mô hình với dữ liệu train\n",
    "    model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=32,verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_and_val(model,X_train,X_test,X_val):\n",
    "    # 10. Dự báo dữ liệu test, val\n",
    "    train_predict=model.predict(X_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_pred_val=model.predict(X_val)\n",
    "    return train_predict, y_pred, y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_data(train_predict,y_pred,y_pred_val,scaler):\n",
    "    # 11. Chuẩn hóa dữ liệu y_pred, y_pred_val\n",
    "    train_predict=scaler.inverse_transform(train_predict)\n",
    "    y_pred=scaler.inverse_transform(y_pred)\n",
    "    y_pred_val=scaler.inverse_transform(y_pred_val)\n",
    "    return train_predict, y_pred, y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mda_cal(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return np.mean((np.sign(actual[1:] - actual[:-1]) == np.sign(predicted[1:] - actual[:-1])).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviation(ytest, pred, scaler):\n",
    "  test=scaler.inverse_transform(ytest.reshape(-1,1))\n",
    "  rmse=np.sqrt(mean_squared_error(test,pred))\n",
    "  print(f\"RMSE: {rmse:.2f}\")\n",
    "  mape=mean_absolute_percentage_error(test,pred)\n",
    "  print(f\"MAPE: {mape*100:.2f}%\")\n",
    "  mda = mda_cal(test, pred)\n",
    "  print(f\"MDA: {mda:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict30days(val_data, model):\n",
    "    val_len=len(val_data)\n",
    "    # 13. Dự báo 30 ngày tiếp theo\n",
    "    x_input=val_data[60:].reshape(1,-1)\n",
    "    x_input.shape\n",
    "\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "\n",
    "    from numpy import array\n",
    "\n",
    "    lst_output=[]\n",
    "    n_steps=val_len-60\n",
    "    i=0\n",
    "    while(i<30):\n",
    "        \n",
    "        if(len(temp_input)>n_steps):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            print(\"{} day input {}\".format(i,x_input))\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            print(len(temp_input))\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "    return lst_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "def visualize_overview(df,train_size,test_size,val_size,train_data,test_data,val_data,y_pred,y_pred_val,scaler):\n",
    "    #train\n",
    "    plt.figure(figsize=(12,6))\n",
    "    train_index = df[:train_size].index\n",
    "    plt.plot(train_index,scaler.inverse_transform(train_data))\n",
    "    #test\n",
    "    test_index = df[train_size:train_size+test_size].index\n",
    "    plt.plot(test_index,scaler.inverse_transform(test_data))\n",
    "    #test predict\n",
    "    test_predict_index = df[train_size+41:train_size+test_size].index\n",
    "    plt.plot(test_predict_index,(y_pred))\n",
    "    #val\n",
    "    val_index = df[train_size+test_size:train_size+test_size+val_size].index\n",
    "    plt.plot(val_index,scaler.inverse_transform(val_data))\n",
    "    #val predict\n",
    "    val_predict_index = df[train_size+test_size+41:train_size+test_size+val_size].index\n",
    "    plt.plot(val_predict_index,y_pred_val)\n",
    "    #prediect_data_index = pd.RangeIndex(start=len(df1)-1, stop=len(df1)+29, step=1)\n",
    "    #plt.plot(prediect_data_index,scaler.inverse_transform(lst_output))\n",
    "    #plt.legend(['Train','Test','Predict','Validate','ValidatePred','Predict30days'])\n",
    "    plt.legend(['Train','Test','Predict','Validate','ValidatePred'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm hàm tổng quát nhận vào tên ngân hàng và tỉ lệ train, test, val, time_step, unit_model\n",
    "def RNN_with_bank_and_ratio(bank,train_ratio,test_ratio,val_ratio,time_step,unit_model):\n",
    "     #in markdown để biết đang chạy ngân hàng nào và tỉ lệ chia tập train/test/validation nào\n",
    "     display(Markdown('## '+bank+' '+str(train_ratio*10)[0]+'-'+str(test_ratio*10)[0]+'-'+str(val_ratio*10)[0]))\n",
    "\n",
    "     #Đọc dữ liệu từ file csv\n",
    "     df=data_preprocessing(bank)\n",
    "\n",
    "     #scale dữ liệu\n",
    "     scaler=MinMaxScaler(feature_range=(0,1))\n",
    "     df1=scale_data(df,scaler)\n",
    "\n",
    "     #chia dữ liệu thành 3 tập train, test, val\n",
    "     train_size,test_size,val_size=get_split_size(df1,train_ratio,test_ratio)\n",
    "     train_data,test_data,val_data=split_data(df1,train_size,test_size)\n",
    "\n",
    "     #reshape dữ liệu\n",
    "     X_train, y_train, X_test, ytest, X_val, yval=reshape_data(train_data,test_data,val_data,time_step)\n",
    "\n",
    "     #tạo model\n",
    "     model=build_model(time_step,unit_model)\n",
    "\n",
    "     #fit model\n",
    "     model=fit(model,X_train,y_train,X_test,ytest)\n",
    "\n",
    "     #Tạo tên đồ thị\n",
    "     title='RNN '+bank+' '+str(train_ratio*10)[0]+'-'+str(test_ratio*10)[0]+'-'+str(val_ratio*10)[0]\n",
    "\n",
    "     #Dự báo dữ liệu test, val\n",
    "     train_predict, y_pred, y_pred_val=predict_test_and_val(model,X_train,X_test,X_val)\n",
    "\n",
    "     #Chuẩn hóa dữ liệu y_pred, y_pred_val\n",
    "     train_predict, y_pred, y_pred_val=inverse_data(train_predict, y_pred, y_pred_val,scaler)\n",
    "\n",
    "     #Vẽ biểu đồ\n",
    "     visualize_overview(df,train_size,test_size,val_size,train_data,test_data,val_data,y_pred,y_pred_val,scaler)\n",
    "\n",
    "     #Đánh giá độ chính xác  \n",
    "     print('Test set')\n",
    "     deviation(ytest,y_pred,scaler)\n",
    "     print('--------')\n",
    "     print('Validation set')\n",
    "     deviation(yval,y_pred_val,scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
